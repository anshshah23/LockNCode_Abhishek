# -*- coding: utf-8 -*-
"""URL-Based Phishing Detection Using Machine Learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-eRrKJB2ePi89XTTKwLTrk0JZe98cpXA

Phishing Means Sending Fraud commucations that appear to come from a reputable Source.

       The Internet has become an indispensable part of our life, However, It also has provided opportunities to anonymously perform malicious activities like Phishing. Phishers try to deceive their victims by social engineering or creating mockup websites to steal information such as account ID, username, password from individuals and organizations. Although many methods have been proposed to detect phishing websites, Phishers have evolved their methods to escape from these detection methods. One of the most successful methods for detecting these malicious activities is Machine Learning. This is because most Phishing attacks have some common characteristics which can be identified by machine learning methods.

# Loading required Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
from sklearn import metrics
import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv("/content/phishing.csv")

df.head()

df['class'].value_counts()

df.shape

df.columns

df.info()

"""From the above we can conclude that all the columns data is integer"""

df.describe()

df.nunique()

df.isnull().sum()

"""There is no null values in our dataset"""

df = df.drop(['Index'], axis = 1)

df.describe().T

df.duplicated().sum()

"""we had duplicate data in our dataset so we have to remove it."""

df.drop_duplicates(inplace=True)

df.duplicated().sum()

df.shape

"""OBSERVATIONS:
1. There are 5849 instances and 31 fearures in dataset.
2. Out of which 30 are independent features where as 1 is dependent feature.
3. Each feature is in int datatype, so there is no need to use LabelEncoder.
4. There is no missing value in dataset.
"""

clmns=['class','HTTPS','SubDomains']
for i in clmns:
    print(f"The Value counts of {i} :")
    print(df[i].value_counts().to_string(),'\n')

"""From the above we can understand that our data set is imbalanced

# EDA

### Univariate
"""

df['class'].value_counts().plot(kind='pie',autopct='%1.2f%%')
plt.title("Phishing Count")
plt.show()

"""phishing count is more than the non phishing count"""

sns.distplot( a=df["class"], hist=True, kde=False, rug=False )

"""Phishing count is more here also as the class which has 1 is greater than the class which has -1"""

sns.kdeplot(df['class'])

count=sns.countplot(x=df.HTTPS,data=df)
for i in count.containers:
    count.bar_label(i)
plt.title("phishing count of HTTPS",color='y',size=20,loc='left')
plt.show()

"""    phishing URLS are 2128.
    legitimate HTTPS URLS or non_phishing URLS are 3000.
    Suspicious URLS are 721 (0 indicates a potential risk of features that indicate a potential risk of phishing,
    but they are not confirmed phishing URLS).
"""

count=sns.countplot(x=df.AnchorURL,data=df)
for i in count.containers:
    count.bar_label(i)
plt.title("The count of AnchorURL ",color='y',size=20,loc='left')
plt.show()

"""In the AnchorURL suspicious URLS are more than the Non-phishing and phishing URLS"""

count=sns.countplot(x=df.SubDomains,data=df)
for i in count.containers:
    count.bar_label(i)
plt.title("The count of SubDomains ",color='y',size=20,loc='left')
plt.show()

"""count of non phishing URLS is more

### Bivariate
"""

sns.boxplot(x=df.HTTPS,y=df.WebsiteTraffic)
plt.title("HTTPS vs WebsiteTraffic",color='orange',size=20)
plt.show()

sns.boxplot(x=df.HTTPS,y=df.SubDomains)
plt.title("HTTPS vs SubDomains",color='y',size=20)
plt.show()

"""There are outliers are in my dataset"""

sns.distplot( a=df["class"], hist=True, kde=False, rug=False )

"""count of phishing URLS is more"""

sns.distplot( a=df["HTTPS"], hist=True, kde=False, rug=False )

"""count of phishing URLS is less in case of HTTPS

### Multivariate
"""

plt.figure(figsize=(15,15))
sns.heatmap(df.corr(), annot=True)
plt.show()

df.corr()

"""here in this dataset some features have no good reationship so we can delete those

we can delete the columns based on dependent variable class
 So, here I'm going to delete LongURL,ShortURL,Symbol@, Redirecting //,DomainRegLen, Favicon, UsingPopupWindow, IframeRedirection, LinksPointingToPage
"""

df.drop(columns=['LongURL','Symbol@','ShortURL','Redirecting//','DomainRegLen', 'Favicon', 'UsingPopupWindow', 'IframeRedirection', 'LinksPointingToPage'],inplace=True)

df.head()

df.shape

"""### Handling Outliers"""

for i in df.columns:
    if type(df[i][0])!=str:
        sns.boxplot(df[i])
        plt.title(i)
        plt.show()

"""We had outliers in our dataset
'PrefixSuffix-','NonStdPort', 'HTTPSDomainURL','AnchorURL','ServerFormHandler','InfoEmail','AbnormalURL','WebsiteForwarding','StatusBarCust','DisableRightClick','GoogleIndex','StatsReport'

"""

# Z-score method
z_scores = np.abs((df - df.mean()) / df.std())
outliers = df[z_scores > 3]

# IQR method
Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1
outliers = df[(df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))]

#Handle outliers
# Remove outliers
df_cleaned = df.drop(outliers.index)

# Replace outliers with median
df_cleaned = df.copy()
df_cleaned=np.where(z_scores>3,df.median(),df)

# Apply log transformation
df_cleaned = np.log1p(df+0.0001)

# Validate the results
print(df_cleaned.describe())

df_cleaned.hist()
plt.show()

# Verify the results using box plots

plt.figure(figsize=(10, 6))


# Original data

plt.subplot(1, 2, 1)

plt.boxplot(df.values)

plt.title('Original Data')


# Cleaned data

plt.subplot(1, 2, 2)

plt.boxplot(df_cleaned.values)

plt.title('Cleaned Data')

plt.show()

"""# Splitting the data
The data is split into train & test sets, 80-20 split.
"""

X = df.drop(["class"],axis =1)
y = df["class"]

"""## Scaling"""

from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
clmn_names=X.columns
scaled=scaler.fit_transform(X)
features=pd.DataFrame(scaled,columns=clmn_names)
features

"""## Train Test and Split"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(features, y, test_size = 0.2, random_state = 42)
X_train.shape, y_train.shape, X_test.shape, y_test.shape

X_train.shape

y_train.shape

X_test.shape

y_test.shape

pip install imblearn

from imblearn.over_sampling import SMOTE
smote=SMOTE(sampling_strategy=1)
X_train,y_train=smote.fit_resample(X_train,y_train)
pd.DataFrame(y_train).value_counts()

"""## Model Building & Training:
   Supervised machine learning is one of the most commonly used and successful types of machine learning. Supervised learning is used whenever we want to predict a certain outcome/label from a given set of features, and we have examples of features-label pairs. We build a machine learning model from these features-label pairs, which comprise our training set. Our goal is to make accurate predictions for new, never-before-seen data.

   There are two major types of supervised machine learning problems, called classification and regression. Our data set comes under regression problem, as the prediction of suicide rate is a continuous number, or a floating-point number in programming terms. The supervised machine learning models (regression) considered to train the dataset in this notebook are:

1. Logistic Regression
2. k-Nearest Neighbors
3. Naive Bayes
4. Decision Tree
5. Random Forest
6. Gradient Boosting
7. Multi Layer perceptron Classifier
8. Support Vector Machine Classifier

  The metrics considered to evaluate the model performance are Accuracy & F1 score, confusion matrix.

## Logistic Regression

Logistic regression predicts the output of a categorical dependent variable. Therefore the outcome must be a categorical or discrete value. Logistic Regression is much similar to the Linear Regression except that how they are used. Linear Regression is used for solving Regression problems, whereas Logistic regression is used for solving the classification problems.
"""

from sklearn.linear_model import LogisticRegression
# instantiate the model
log = LogisticRegression()
# fit the model
Log=log.fit(X_train,y_train)
predict=Log.predict(X_test)

from sklearn.metrics import accuracy_score,confusion_matrix,classification_report

accuracy_score(y_test,predict)

confusion_matrix(y_test,predict)

"""computing the classification report of the model"""

print(classification_report(y_test,predict))

"""##  K-Nearest Neighbors : Classifier

K-Nearest Neighbour is one of the simplest Machine Learning algorithms based on Supervised Learning technique. K-NN algorithm assumes the similarity between the new case/data and available cases and put the new case into the category that is most similar to the available categories.
"""

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
Knn=knn.fit(X_train,y_train)
k_predict=Knn.predict(X_test)

accuracy_score(y_test,k_predict)

confusion_matrix(y_test,k_predict)

print(classification_report(y_test,k_predict))

"""## Naive Bayes : Classifier

Naïve Bayes algorithm is a supervised learning algorithm, which is based on Bayes theorem and used for solving classification problems.It is mainly used in text, image classification that includes a high-dimensional training dataset. Naïve Bayes Classifier is one of the simple and most effective Classification algorithms which helps in building the fast machine learning models that can make quick predictions.
"""

# Naive Bayes Classifier Model
from sklearn.naive_bayes import GaussianNB

# instantiate the model
nb = GaussianNB()

# fit the model
Nb=nb.fit(X_train,y_train)
n_predict=nb.predict(X_test)

accuracy_score(y_test,n_predict)

confusion_matrix(y_test,n_predict)

print(classification_report(y_test,n_predict))

"""##  Decision Trees : Classifier

Decision Tree is a Supervised learning technique that can be used for both classification and Regression problems, but mostly it is preferred for solving Classification problems. It is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome.
"""

# Decision Tree Classifier model
from sklearn.tree import DecisionTreeClassifier

# instantiate the model
dt = DecisionTreeClassifier(max_depth=30)

# fit the model
Dt=dt.fit(X_train, y_train)
d_predict=Dt.predict(X_test)

accuracy_score(y_test,d_predict)

confusion_matrix(y_test,d_predict)

print(classification_report(y_test,d_predict))

"""##  Random Forest : Classifier

Random Forest is a popular machine learning algorithm that belongs to the supervised learning technique. It can be used for both Classification and Regression problems in ML. It is based on the concept of ensemble learning, which is a process of combining multiple classifiers to solve a complex problem and to improve the performance of the model.
"""

# Random Forest Classifier Model
from sklearn.ensemble import RandomForestClassifier

# instantiate the model
forest = RandomForestClassifier(n_estimators=10)

# fit the model
Forest=forest.fit(X_train,y_train)
f_predict=Forest.predict(X_test)

accuracy_score(y_test,f_predict)

confusion_matrix(y_test,f_predict)

print(classification_report(y_test,f_predict))

from sklearn.model_selection import cross_val_score
cv=cross_val_score(Forest,X_train,y_train,cv=11)
np.mean(cv)

"""## Gradient Boosting Classifier
Gradient boosting classifiers are a group of machine learning algorithms that combine many weak learning models together to
create a strong predictive model.Decision trees are usually used when doing gradient boosting.
Boosting algorithms play a crucial role in dealing with bias variance trade-off.  
Unlike bagging algorithms, which only controls for high variance in a model, boosting controls both the aspects (bias & variance),
and is considered to be more effective.
"""

# Gradient Boosting Classifier Model
from sklearn.ensemble import GradientBoostingClassifier

# instantiate the model
gbc = GradientBoostingClassifier(max_depth=4,learning_rate=0.7)

# fit the model
Gbc=gbc.fit(X_train,y_train)
g_predict=Gbc.predict(X_test)

accuracy_score(y_test,g_predict)

confusion_matrix(y_test,g_predict)

print(classification_report(y_test, g_predict))

"""##  Multi-layer Perceptron classifier

MLPClassifier stands for Multi-layer Perceptron classifier which in the name itself connects to a Neural Network. Unlike other classification algorithms such as Support Vectors or Naive Bayes Classifier, MLPClassifier relies on an underlying Neural Network to perform the task of classification.

"""

# Multi-layer Perceptron Classifier Model
from sklearn.neural_network import MLPClassifier

# instantiate the model
mlp = MLPClassifier()
#mlp = GridSearchCV(mlpc, parameter_space)

# fit the model
Mlp=mlp.fit(X_train,y_train)
m_predict=Mlp.predict(X_test)

accuracy_score(y_test,m_predict)

confusion_matrix(y_test,m_predict)

print(classification_report(y_test,m_predict))

"""##   Support Vector Machine : Classifier

Support Vector Machine or SVM is one of the most popular Supervised Learning algorithms, which is used for Classification as well as Regression problems.
The goal of the SVM algorithm is to create the best line or decision boundary that can segregate n-dimensional space into
classes so that we can easily put the new data point in the correct category in the future.
"""

# Support Vector Classifier model
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV

# defining parameter range
param_grid = {'gamma': [0.1],'kernel': ['rbf','linear']}

svc = GridSearchCV(SVC(), param_grid)

# fitting the model for grid search
Svc=svc.fit(X_train, y_train)
s_predict=Svc.predict(X_test)

accuracy_score(y_test,s_predict)

confusion_matrix(y_test,s_predict)

print(classification_report(y_test, s_predict))

"""##  Comparision of Models
To compare the models performance, a dataframe is created. The columns of this dataframe are the lists created to store the results of the model.
"""

# Instantiate the models
logistic_regression = LogisticRegression()
knn = KNeighborsClassifier()
naive_bayes = GaussianNB()
decision_tree = DecisionTreeClassifier(max_depth=30)
random_forest = RandomForestClassifier(n_estimators=10)
gradient_boosting = GradientBoostingClassifier(max_depth=4, learning_rate=0.7)
mlp = MLPClassifier()
svc = SVC()

# Train and predict for each model
models = [
    ("Logistic Regression", logistic_regression),
    ("K-Nearest Neighbors", knn),
    ("Naive Bayes", naive_bayes),
    ("Decision Tree", decision_tree),
    ("Random Forest", random_forest),
    ("Gradient Boosting", gradient_boosting),
    ("Multi-Layer Perceptron", mlp),
    ("Support Vector", svc)
]

for model_name, model in models:
    print(f"Model: {model_name}")

    # Fit the model
    model.fit(X_train, y_train)

    # Predict
    y_pred = model.predict(X_test)

    # Evaluation metrics
    accuracy = accuracy_score(y_test, y_pred)
    confusion = confusion_matrix(y_test, y_pred)
    classification = classification_report(y_test, y_pred)

    print(f"Accuracy: {accuracy}")
    print(f"Confusion Matrix:\n{confusion}")
    print(f"Classification Report:\n{classification}")
    print("-" * 50)

"""Printing in sorted order"""

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC

logistic_regression = LogisticRegression()
knn = KNeighborsClassifier()
naive_bayes = GaussianNB()
decision_tree = DecisionTreeClassifier(max_depth=30)
random_forest = RandomForestClassifier(n_estimators=10)
gradient_boosting = GradientBoostingClassifier(max_depth=4, learning_rate=0.7)
mlp = MLPClassifier()
svc = SVC()

# Train and predict for each model
models = [
    ("Logistic Regression", logistic_regression),
    ("K-Nearest Neighbors", knn),
    ("Naive Bayes", naive_bayes),
    ("Decision Tree", decision_tree),
    ("Random Forest", random_forest),
    ("Gradient Boosting", gradient_boosting),
    ("Multi-Layer Perceptron", mlp),
    ("Support Vector", svc)
]

results = []

for model_name, model in models:
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    results.append((model_name, model, accuracy))

# Sort results based on accuracy
results.sort(key=lambda x: x[2], reverse=True)

for model_name, model, accuracy in results:
    print(f"Model: {model_name}")
    print(f"Accuracy: {accuracy}")
    y_pred = model.predict(X_test)
    confusion = confusion_matrix(y_test, y_pred)
    classification = classification_report(y_test, y_pred)
    print(f"Confusion Matrix:\n{confusion}")
    print(f"Classification Report:\n{classification}")
    print("-" * 50)

"""## Testing the Model"""

X_train.head(20)

X_train.iloc[480]

Gbc.predict([[1,-1,1,1,1,1,1,1,0,1,1,1,1,1,1,-1,1,1,-1,1,1]])

"""## Storing Best Model"""

import pickle

# dump information to that file
pickle.dump(Gbc, open('Phishing_model.pkl', 'wb'))

"""##  Conclusion

1. The final take away form this project is to explore various machine learning models, perform Exploratory Data Analysis on phishing dataset and understanding their features.
2. Creating this notebook helped me to learn a lot about the features affecting the models to detect whether URL is safe or not, also I came to know how to tuned model and how they affect the model performance.
3. The final conclusion on the Phishing dataset is that the some feature like "HTTTPS", "AnchorURL", "WebsiteTraffic" have more importance to classify URL is phishing URL or not.
4. Gradient Boosting Classifier correctly classify URL upto 94.52% respective classes and hence reduces the chance of malicious attachments.
"""

